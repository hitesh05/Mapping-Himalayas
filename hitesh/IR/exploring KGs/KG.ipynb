{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from stanfordnlp.server import CoreNLPClient\n",
    "# import stanfordnlp\n",
    "# # stanfordnlp.download('en', force=True)\n",
    "# # exit(0)\n",
    "# import os\n",
    "# os.environ['CORENLP_HOME'] = 'corenlp_server-16e532541d164bf6.props'\n",
    "\n",
    "# # Open text file and extract text\n",
    "# path = '/home/hitesh/Documents/IIIT-H/Honours 2/IR/text_files/1810 Survey for Discovering Sources of the Ganges by Raper from ARv11 s.txt'\n",
    "# with open(path, 'r') as file:\n",
    "#     text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyngrok import ngrok\n",
    "\n",
    "# # Start the CoreNLP server\n",
    "# !java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "\n",
    "# # Create a public URL for the server\n",
    "# url = ngrok.connect(9000, 'http')\n",
    "# print(\"Public URL:\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server_url = 'http://37dd-218-185-248-66.ngrok.io'\n",
    "# with CoreNLPClient(annotators=['openie'], server=server_url, timeout=30000, memory='1G') as client:\n",
    "#     # Extract OpenIE triples\n",
    "#     triples = []\n",
    "#     for sentence in text.split('.'):\n",
    "#         if sentence:\n",
    "#             ann = client.annotate(sentence)\n",
    "#             print('here')\n",
    "#             for s in ann.sentences:\n",
    "#                 for triple in s.openie_triples:\n",
    "#                     triples.append(triple)\n",
    "                    \n",
    "# # Create knowledge graph dictionary\n",
    "# knowledge_graph = {}\n",
    "# for triple in triples:\n",
    "#     subject = triple[0]\n",
    "#     relation = triple[1]\n",
    "#     object = triple[2]\n",
    "#     if subject not in knowledge_graph:\n",
    "#         knowledge_graph[subject] = {}\n",
    "#     if relation not in knowledge_graph[subject]:\n",
    "#         knowledge_graph[subject][relation] = []\n",
    "#     knowledge_graph[subject][relation].append(object)\n",
    "\n",
    "# # Print knowledge graph\n",
    "# print(knowledge_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_ewt_lemmatizer.pt  en_ewt.pretrain.pt  en_ewt_tokenizer.pt\n",
      "en_ewt_parser.pt      en_ewt_tagger.pt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/hitesh/stanfordnlp_resources/en_ewt_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: openie\n",
      "With settings: \n",
      "{'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'openie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d3a7db9bb4ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set up the CoreNLP client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorenlp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/hitesh/stanfordnlp_resources/en_ewt_models/\"\u001b[0m \u001b[0;31m# replace with the actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcorenlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanfordnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openie\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorenlp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define the text to extract relations from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/stanfordnlp/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processors, lang, models_dir, treebank, use_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# try to build processor, throw an exception if there is a requirements issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n\u001b[0m\u001b[1;32m    122\u001b[0m                                                                                           \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                                                                                           use_gpu=self.use_gpu)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'openie'"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "# Set up the CoreNLP client\n",
    "corenlp_dir = \"/home/hitesh/stanfordnlp_resources/en_ewt_models/\" # replace with the actual path\n",
    "corenlp = stanfordnlp.Pipeline(processors=\"openie\", lang=\"en\", models_dir=corenlp_dir)\n",
    "\n",
    "# Define the text to extract relations from\n",
    "text = \"Barack Obama was born in Hawaii. He was elected president of the United States in 2008.\"\n",
    "\n",
    "# Process the text with the CoreNLP client\n",
    "doc = corenlp(text)\n",
    "\n",
    "# Extract the relations from the processed text\n",
    "for sent in doc.sentences:\n",
    "    for triple in sent.openie_triples:\n",
    "        print(triple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
